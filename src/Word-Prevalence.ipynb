{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age vs Word-Prevalence Metrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm/Steps:\n",
    "# step-1 :: Read blog authorship corpora \n",
    "#\n",
    "# step-2 :: Clean blog authorship corpora\n",
    "#          (a) Remove xml tags such as <Blog>, <post>, <date>\n",
    "#          (b) Remove newline and tab characters\n",
    "#          (c) Tokenize blog text\n",
    "#          (d) Convert to lower-case\n",
    "#          (e) Remove punctuation\n",
    "#          (f) Remove non-alphabetic tokens\n",
    "#          (g) Remove stopwords\n",
    "#          (h) Convert to pandas dataframe for further processing\n",
    "#\n",
    "# step-3 :: Read word-prevalnce dataset\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tofii/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read and prepare the blogs-authorship corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename,'r', encoding = 'ISO-8859-1') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "    \n",
    "def clean_data(data):    \n",
    "    ''' \n",
    "    Function to clean the data.\n",
    "    '''\n",
    "    # Remove XML tags\n",
    "    data = data.replace('<Blog>','')\n",
    "    data = data.replace('</Blog>','')\n",
    "    data = data.replace('<post>','')\n",
    "    data = data.replace('</post>','')\n",
    "    data = data.replace('<date>','')\n",
    "    data = data.replace('</date>','')\n",
    "    \n",
    "    # Remove newline and tab characters\n",
    "    data = data.replace('\\n','')\n",
    "    data = data.replace('\\t','')\n",
    "    \n",
    "    # Tokenize text with nltk\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    \n",
    "    # Convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # Remove punctuations\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/19320 blogs processed\n",
      "200/19320 blogs processed\n",
      "300/19320 blogs processed\n",
      "400/19320 blogs processed\n",
      "500/19320 blogs processed\n",
      "600/19320 blogs processed\n",
      "700/19320 blogs processed\n",
      "800/19320 blogs processed\n",
      "900/19320 blogs processed\n",
      "1000/19320 blogs processed\n",
      "1100/19320 blogs processed\n",
      "1200/19320 blogs processed\n",
      "1300/19320 blogs processed\n",
      "1400/19320 blogs processed\n",
      "1500/19320 blogs processed\n",
      "1600/19320 blogs processed\n",
      "1700/19320 blogs processed\n",
      "1800/19320 blogs processed\n",
      "1900/19320 blogs processed\n",
      "2000/19320 blogs processed\n",
      "2100/19320 blogs processed\n",
      "2200/19320 blogs processed\n",
      "2300/19320 blogs processed\n",
      "2400/19320 blogs processed\n",
      "2500/19320 blogs processed\n",
      "2600/19320 blogs processed\n",
      "2700/19320 blogs processed\n",
      "2800/19320 blogs processed\n",
      "2900/19320 blogs processed\n",
      "3000/19320 blogs processed\n",
      "3100/19320 blogs processed\n",
      "3200/19320 blogs processed\n",
      "3300/19320 blogs processed\n",
      "3400/19320 blogs processed\n",
      "3500/19320 blogs processed\n",
      "3600/19320 blogs processed\n",
      "3700/19320 blogs processed\n",
      "3800/19320 blogs processed\n",
      "3900/19320 blogs processed\n",
      "4000/19320 blogs processed\n",
      "4100/19320 blogs processed\n",
      "4200/19320 blogs processed\n",
      "4300/19320 blogs processed\n",
      "4400/19320 blogs processed\n",
      "4500/19320 blogs processed\n",
      "4600/19320 blogs processed\n",
      "4700/19320 blogs processed\n",
      "4800/19320 blogs processed\n",
      "4900/19320 blogs processed\n",
      "5000/19320 blogs processed\n",
      "5100/19320 blogs processed\n",
      "5200/19320 blogs processed\n",
      "5300/19320 blogs processed\n",
      "5400/19320 blogs processed\n",
      "5500/19320 blogs processed\n",
      "5600/19320 blogs processed\n",
      "5700/19320 blogs processed\n",
      "5800/19320 blogs processed\n",
      "5900/19320 blogs processed\n",
      "6000/19320 blogs processed\n",
      "6100/19320 blogs processed\n",
      "6200/19320 blogs processed\n",
      "6300/19320 blogs processed\n",
      "6400/19320 blogs processed\n",
      "6500/19320 blogs processed\n",
      "6600/19320 blogs processed\n",
      "6700/19320 blogs processed\n",
      "6800/19320 blogs processed\n",
      "6900/19320 blogs processed\n",
      "7000/19320 blogs processed\n",
      "7100/19320 blogs processed\n",
      "7200/19320 blogs processed\n",
      "7300/19320 blogs processed\n",
      "7400/19320 blogs processed\n",
      "7500/19320 blogs processed\n",
      "7600/19320 blogs processed\n",
      "7700/19320 blogs processed\n",
      "7800/19320 blogs processed\n",
      "7900/19320 blogs processed\n",
      "8000/19320 blogs processed\n",
      "8100/19320 blogs processed\n",
      "8200/19320 blogs processed\n",
      "8300/19320 blogs processed\n",
      "8400/19320 blogs processed\n",
      "8500/19320 blogs processed\n",
      "8600/19320 blogs processed\n",
      "8700/19320 blogs processed\n",
      "8800/19320 blogs processed\n",
      "8900/19320 blogs processed\n",
      "9000/19320 blogs processed\n",
      "9100/19320 blogs processed\n",
      "9200/19320 blogs processed\n",
      "9300/19320 blogs processed\n",
      "9400/19320 blogs processed\n",
      "9500/19320 blogs processed\n",
      "9600/19320 blogs processed\n",
      "9700/19320 blogs processed\n",
      "9800/19320 blogs processed\n",
      "9900/19320 blogs processed\n",
      "10000/19320 blogs processed\n",
      "10100/19320 blogs processed\n",
      "10200/19320 blogs processed\n",
      "10300/19320 blogs processed\n",
      "10400/19320 blogs processed\n",
      "10500/19320 blogs processed\n",
      "10600/19320 blogs processed\n",
      "10700/19320 blogs processed\n",
      "10800/19320 blogs processed\n",
      "10900/19320 blogs processed\n",
      "11000/19320 blogs processed\n",
      "11100/19320 blogs processed\n",
      "11200/19320 blogs processed\n",
      "11300/19320 blogs processed\n",
      "11400/19320 blogs processed\n",
      "11500/19320 blogs processed\n",
      "11600/19320 blogs processed\n",
      "11700/19320 blogs processed\n",
      "11800/19320 blogs processed\n",
      "11900/19320 blogs processed\n",
      "12000/19320 blogs processed\n",
      "12100/19320 blogs processed\n",
      "12200/19320 blogs processed\n",
      "12300/19320 blogs processed\n",
      "12400/19320 blogs processed\n",
      "12500/19320 blogs processed\n",
      "12600/19320 blogs processed\n",
      "12700/19320 blogs processed\n",
      "12800/19320 blogs processed\n",
      "12900/19320 blogs processed\n",
      "13000/19320 blogs processed\n",
      "13100/19320 blogs processed\n",
      "13200/19320 blogs processed\n",
      "13300/19320 blogs processed\n",
      "13400/19320 blogs processed\n",
      "13500/19320 blogs processed\n",
      "13600/19320 blogs processed\n",
      "13700/19320 blogs processed\n",
      "13800/19320 blogs processed\n",
      "13900/19320 blogs processed\n",
      "14000/19320 blogs processed\n",
      "14100/19320 blogs processed\n",
      "14200/19320 blogs processed\n",
      "14300/19320 blogs processed\n",
      "14400/19320 blogs processed\n",
      "14500/19320 blogs processed\n",
      "14600/19320 blogs processed\n",
      "14700/19320 blogs processed\n",
      "14800/19320 blogs processed\n",
      "14900/19320 blogs processed\n",
      "15000/19320 blogs processed\n",
      "15100/19320 blogs processed\n",
      "15200/19320 blogs processed\n",
      "15300/19320 blogs processed\n",
      "15400/19320 blogs processed\n",
      "15500/19320 blogs processed\n",
      "15600/19320 blogs processed\n",
      "15700/19320 blogs processed\n",
      "15800/19320 blogs processed\n",
      "15900/19320 blogs processed\n",
      "16000/19320 blogs processed\n",
      "16100/19320 blogs processed\n",
      "16200/19320 blogs processed\n",
      "16300/19320 blogs processed\n",
      "16400/19320 blogs processed\n",
      "16500/19320 blogs processed\n",
      "16600/19320 blogs processed\n",
      "16700/19320 blogs processed\n",
      "16800/19320 blogs processed\n",
      "16900/19320 blogs processed\n",
      "17000/19320 blogs processed\n",
      "17100/19320 blogs processed\n",
      "17200/19320 blogs processed\n",
      "17300/19320 blogs processed\n",
      "17400/19320 blogs processed\n",
      "17500/19320 blogs processed\n",
      "17600/19320 blogs processed\n",
      "17700/19320 blogs processed\n",
      "17800/19320 blogs processed\n",
      "17900/19320 blogs processed\n",
      "18000/19320 blogs processed\n",
      "18100/19320 blogs processed\n",
      "18200/19320 blogs processed\n",
      "18300/19320 blogs processed\n",
      "18400/19320 blogs processed\n",
      "18500/19320 blogs processed\n",
      "18600/19320 blogs processed\n",
      "18700/19320 blogs processed\n",
      "18800/19320 blogs processed\n",
      "18900/19320 blogs processed\n",
      "19000/19320 blogs processed\n",
      "19100/19320 blogs processed\n",
      "19200/19320 blogs processed\n",
      "19300/19320 blogs processed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>INDUSTRY</th>\n",
       "      <th>ASTROLOGICAL SIGN</th>\n",
       "      <th>POST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4162441</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>[destiny, might, say, anything, hear, chosen, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3489929</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>Student</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>[long, time, coming, made, serious, decisions,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3954575</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>BusinessServices</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>[sit, work, three, hours, left, guess, bad, ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3364931</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>[today, normal, nothing, much, talk, except, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3162067</td>\n",
       "      <td>female</td>\n",
       "      <td>24</td>\n",
       "      <td>Education</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>[feel, water, crystal, vibrations, mother, bea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  GENDER AGE          INDUSTRY ASTROLOGICAL SIGN  \\\n",
       "0  4162441    male  16           Student       Sagittarius   \n",
       "1  3489929  female  25           Student            Cancer   \n",
       "2  3954575  female  23  BusinessServices            Gemini   \n",
       "3  3364931    male  16           Student             Virgo   \n",
       "4  3162067  female  24         Education            Cancer   \n",
       "\n",
       "                                                POST  \n",
       "0  [destiny, might, say, anything, hear, chosen, ...  \n",
       "1  [long, time, coming, made, serious, decisions,...  \n",
       "2  [sit, work, three, hours, left, guess, bad, ti...  \n",
       "3  [today, normal, nothing, much, talk, except, g...  \n",
       "4  [feel, water, crystal, vibrations, mother, bea...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir('blogs_data')\n",
    "posts = []\n",
    "header = ['ID','GENDER','AGE','INDUSTRY','ASTROLOGICAL SIGN','POST']\n",
    "\n",
    "counter = 0\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    attribs = file.split('.')[0:-1]\n",
    "    \n",
    "    # Read file\n",
    "    post = read_file('blogs_data/'+file)\n",
    "    \n",
    "    # Clean post\n",
    "    cleaned_post = clean_data(post)\n",
    "    \n",
    "    # Add post to attribs list\n",
    "    attribs.append(cleaned_post)\n",
    "    \n",
    "    # Append post and other data to posts list\n",
    "    posts.append(attribs)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print('{}/{} blogs processed'.format(counter, len(files)))\n",
    "# Prepare pandas dataframe\n",
    "posts_df = pd.DataFrame(posts, columns=header)\n",
    "\n",
    "# Print sample\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read word-prevalence corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pknown</th>\n",
       "      <th>Nobs</th>\n",
       "      <th>Prevalence</th>\n",
       "      <th>FreqZipfUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>0.98</td>\n",
       "      <td>438</td>\n",
       "      <td>1.917</td>\n",
       "      <td>7.309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>0.96</td>\n",
       "      <td>434</td>\n",
       "      <td>1.684</td>\n",
       "      <td>2.634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aardwolf</td>\n",
       "      <td>0.21</td>\n",
       "      <td>428</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>1.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abaca</td>\n",
       "      <td>0.24</td>\n",
       "      <td>396</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>1.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aback</td>\n",
       "      <td>0.86</td>\n",
       "      <td>343</td>\n",
       "      <td>1.077</td>\n",
       "      <td>2.496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Pknown  Nobs  Prevalence  FreqZipfUS\n",
       "0         a    0.98   438       1.917       7.309\n",
       "1  aardvark    0.96   434       1.684       2.634\n",
       "2  aardwolf    0.21   428      -0.788       1.292\n",
       "3     abaca    0.24   396      -0.706       1.593\n",
       "4     aback    0.86   343       1.077       2.496"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data as pandas dataframe\n",
    "prevalence_df = pd.read_csv(\"preval_data/prevalence.csv\")\n",
    "\n",
    "# print sample data\n",
    "prevalence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_file(row):\n",
    "    with open('count_matrix.csv','a') as f:\n",
    "        f.writelines(','.join(row))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = posts_df.columns.values.tolist()\n",
    "header.remove('POST')\n",
    "for word in prevalence_df.Word:\n",
    "    if isinstance(word, str) and word not in stop_words:\n",
    "        header.append(word)\n",
    "append_to_file(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for idx, row in posts_df.iterrows():\n",
    "    count_list = row[0:5].values.tolist()\n",
    "    post = row[5]\n",
    "    for word in prevalence_df.Word:\n",
    "        if isinstance(word, str) and word not in stop_words:\n",
    "            cnt = post.count(word)\n",
    "            count_list.append(str(cnt))\n",
    "    append_to_file(count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
