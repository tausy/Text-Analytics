{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used to clean blog data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created: 22 Feb 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import os\n",
    "import xmltodict, json\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def convertXML(name):\n",
    "    \"\"\"\n",
    "    Converts a blog to a list of dates and posts \n",
    "    \n",
    "    name: blog title\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open (\"data/blogs/{}\".format(name), \"r\", encoding = \"ISO-8859-1\") as myfile:\n",
    "        data=myfile.read().replace('\\n', '')\n",
    "\n",
    "    data = data.replace(\"<Blog><date>\",'')\n",
    "    data = data.replace(\"</post></Blog>\",'')\n",
    "    data = data.replace('#','')\n",
    "    data = data.replace(\"</date><post>\",'#')\n",
    "    data = data.replace(\"</post><date>\",'#')\n",
    "\n",
    "    data = data.split('#')\n",
    "    #print(data)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def writeToCSV(author,name,csv_path): \n",
    "    \"\"\"\n",
    "    Returns a list of strings of the for author,age,gender,date,post\n",
    "    \n",
    "    author: author id\n",
    "    name: blog title \n",
    "    csv_path: path to csv\n",
    "    \"\"\"\n",
    "    age = name.split(\".\")[2]\n",
    "    gender = name.split(\".\")[1]\n",
    "\n",
    "    data = convertXML(name)\n",
    "    dates = data[::2]\n",
    "    dates = [d.replace(',','/') for d in dates ]\n",
    "    posts = data[1::2]\n",
    "    with open(csv_path, 'a') as writeFile:\n",
    "        csv.register_dialect('myDialect', delimiter = '#')\n",
    "        writer = csv.writer(writeFile, dialect='myDialect')\n",
    "        \n",
    "        for i in range(len(dates)):\n",
    "            #line = \"#\".join(map(str,[author,age,gender,dates[i],posts[i]]))\n",
    "            row = [author,age,gender,dates[i],posts[i]]\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    writeFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write raw blog data to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19320\n",
      "3489929.female.25.Student.Cancer.xml\n"
     ]
    }
   ],
   "source": [
    "#Read in blog titles\n",
    "titles = os.listdir(\"data/blogs/\")\n",
    "#titles = random.sample(titles, 1000)\n",
    "print(len(titles))\n",
    "print(titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/19320 blogs\n",
      "Processed 1000/19320 blogs\n",
      "Processed 2000/19320 blogs\n",
      "Processed 3000/19320 blogs\n",
      "Processed 4000/19320 blogs\n",
      "Processed 5000/19320 blogs\n",
      "Processed 6000/19320 blogs\n",
      "Processed 7000/19320 blogs\n",
      "Processed 8000/19320 blogs\n",
      "Processed 9000/19320 blogs\n",
      "Processed 10000/19320 blogs\n",
      "Processed 11000/19320 blogs\n",
      "Processed 12000/19320 blogs\n",
      "Processed 13000/19320 blogs\n",
      "Processed 14000/19320 blogs\n",
      "Processed 15000/19320 blogs\n",
      "Processed 16000/19320 blogs\n",
      "Processed 17000/19320 blogs\n",
      "Processed 18000/19320 blogs\n",
      "Processed 19000/19320 blogs\n"
     ]
    }
   ],
   "source": [
    "csv_path = 'data/blogs_raw.csv'\n",
    "\n",
    "#Initialize csv file\n",
    "with open(csv_path, 'w') as writeFile:\n",
    "        csv.register_dialect('myDialect', delimiter = '#')\n",
    "        writer = csv.writer(writeFile, dialect='myDialect')\n",
    "        row = [\"author\",\"age\",\"gender\",\"date\",\"post\"]\n",
    "        writer.writerow(row)\n",
    "            \n",
    "writeFile.close()\n",
    "\n",
    "#write blogs to csv\n",
    "for i,t in enumerate(titles):\n",
    "    writeToCSV(i,t,'data/blogs_raw.csv')\n",
    "    if i%1000 == 0: print(\"Processed {}/19320 blogs\".format(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean CSV post data <br>\n",
    "Every post has been cleaned by:\n",
    "\n",
    "<ul>\n",
    "    <li> Removing extra spaces\n",
    "    <li> Punctuation removed\n",
    "    <li> Made lower case\n",
    "    <li> Stemmed\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pknown</th>\n",
       "      <th>Nobs</th>\n",
       "      <th>Prevalence</th>\n",
       "      <th>FreqZipfUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>0.98</td>\n",
       "      <td>438</td>\n",
       "      <td>1.917</td>\n",
       "      <td>7.309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>0.96</td>\n",
       "      <td>434</td>\n",
       "      <td>1.684</td>\n",
       "      <td>2.634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aardwolf</td>\n",
       "      <td>0.21</td>\n",
       "      <td>428</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>1.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abaca</td>\n",
       "      <td>0.24</td>\n",
       "      <td>396</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>1.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aback</td>\n",
       "      <td>0.86</td>\n",
       "      <td>343</td>\n",
       "      <td>1.077</td>\n",
       "      <td>2.496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Pknown  Nobs  Prevalence  FreqZipfUS\n",
       "0         a    0.98   438       1.917       7.309\n",
       "1  aardvark    0.96   434       1.684       2.634\n",
       "2  aardwolf    0.21   428      -0.788       1.292\n",
       "3     abaca    0.24   396      -0.706       1.593\n",
       "4     aback    0.86   343       1.077       2.496"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevanlence = pd.read_csv(\"data/prevalence.csv\")\n",
    "prevanlence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681288\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>date</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>male</td>\n",
       "      <td>19/August/2004</td>\n",
       "      <td>\\t          DESTINY...       you might not say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>male</td>\n",
       "      <td>17/August/2004</td>\n",
       "      <td>\\t          DEAR ANGEL..      you say it or yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>male</td>\n",
       "      <td>16/August/2004</td>\n",
       "      <td>\\t       MAIN AUR MERI TANHAI (jagjeet singh) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>male</td>\n",
       "      <td>14/August/2004</td>\n",
       "      <td>\\t       mail addressrs(s)  urlLink http://red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>male</td>\n",
       "      <td>09/August/2004</td>\n",
       "      <td>\\t       RAP- ALLRISE so stand back cause u do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author  age gender            date  \\\n",
       "0       0   16   male  19/August/2004   \n",
       "1       0   16   male  17/August/2004   \n",
       "2       0   16   male  16/August/2004   \n",
       "3       0   16   male  14/August/2004   \n",
       "4       0   16   male  09/August/2004   \n",
       "\n",
       "                                                post  \n",
       "0  \\t          DESTINY...       you might not say...  \n",
       "1  \\t          DEAR ANGEL..      you say it or yo...  \n",
       "2  \\t       MAIN AUR MERI TANHAI (jagjeet singh) ...  \n",
       "3  \\t       mail addressrs(s)  urlLink http://red...  \n",
       "4  \\t       RAP- ALLRISE so stand back cause u do...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv(\"data/blogs_raw.csv\",sep=\"#\")\n",
    "print(len(file))\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/blogs_clean.csv\", 'w') as writeFile:\n",
    "    csv.register_dialect('myDialect', delimiter = '#')\n",
    "    writer = csv.writer(writeFile, dialect='myDialect')\n",
    "    row = [\"author\",\"age\",\"gender\",\"date\",\"post\"]\n",
    "    writer.writerow(row)\n",
    "            \n",
    "writeFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author#age#gender#date#post\n",
      "\n",
      "Processed 25000/681288 blogs\n",
      "Processed 50000/681288 blogs\n",
      "Processed 75000/681288 blogs\n",
      "Processed 100000/681288 blogs\n",
      "Processed 125000/681288 blogs\n",
      "Processed 150000/681288 blogs\n",
      "Processed 175000/681288 blogs\n",
      "Processed 200000/681288 blogs\n",
      "Processed 225000/681288 blogs\n",
      "Processed 250000/681288 blogs\n",
      "Processed 275000/681288 blogs\n",
      "Processed 300000/681288 blogs\n",
      "Processed 325000/681288 blogs\n",
      "Processed 350000/681288 blogs\n",
      "Processed 375000/681288 blogs\n",
      "Processed 400000/681288 blogs\n",
      "Processed 425000/681288 blogs\n",
      "Processed 450000/681288 blogs\n",
      "Processed 475000/681288 blogs\n",
      "Processed 500000/681288 blogs\n",
      "Processed 525000/681288 blogs\n",
      "Processed 550000/681288 blogs\n",
      "Processed 575000/681288 blogs\n",
      "Processed 600000/681288 blogs\n",
      "Processed 625000/681288 blogs\n",
      "Processed 650000/681288 blogs\n",
      "Processed 675000/681288 blogs\n"
     ]
    }
   ],
   "source": [
    "with open('data/blogs_raw.csv') as read:  \n",
    "    line = read.readline()\n",
    "    print(line)\n",
    "    line = read.readline()\n",
    "    \n",
    "    with open(\"data/blogs_clean.csv\", 'a') as writeFile:\n",
    "        csv.register_dialect('myDialect', delimiter = '#')\n",
    "        writer = csv.writer(writeFile, dialect='myDialect')\n",
    "    \n",
    "        cnt = 1\n",
    "        while line:\n",
    "            line = line.strip()\n",
    "            line = line.split(\"#\")\n",
    "            post = line[4]\n",
    "            post = cleanPost(post)\n",
    "            row = [line[0],line[1],line[2],line[3],post]\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            line = read.readline()\n",
    "            cnt += 1\n",
    "            if cnt%25000 == 0: print(\"Processed {}/681288 blogs\".format(cnt))\n",
    "                \n",
    "        writeFile.close()\n",
    "        \n",
    "    read.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i make excel lasagna just ask ricotta monkey sorri lisa but that wa your onli contribut after all so much for my great idea to start diet thi weekend posh will kill me i promis to start so we can help each other but consid the fact that the girl is onli look to lose ten that she realli ca afford to i not feel too inclin to encourag her weight loss anyway i keep tell myself that when i get my gazel i will actual stick with it and use it everi day i been lust after one for a year so thi is a realist thought howev know myself i know that i will probabl be for a month and then store it under my bed never to be seen again la vie that me i want to watch tonight i saw it when it came out in theater but due to my bladder i miss a coupl of part of the movi same with harri potter three week ago i need to go see it again becaus of my useless kegel muscl my ball will go to wast of thi i am certain anyway i am wait for lisa to make up her mind as to when she want to start the movi she said earlier that she did want to watch find nemo and lo and behold she show up minut after i had start the movi and watch the whole thing weirdo lmao she just came in here wave a pencil with a peni or pay as she pronounc it eras and yell it a tallywack between her eras and my condom finger puppet we could have a realli kinki adult puppet show go on up in here'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanPost(post):\n",
    "    \n",
    "    tokens = word_tokenize(post)\n",
    "    \n",
    "    tokens = [word for word in tokens if word.isalpha()] #remove punctuation\n",
    "    tokens = [w.lower() for w in tokens] #Lower case\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    tokens = [porter.stem(w) for w in tokens] #Stemmed\n",
    "    post_clean = \" \".join(tokens)\n",
    "    \n",
    "    return post_clean\n",
    "post = file['post'][15]\n",
    "cleanPost(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681288\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author#age#gender#date#post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0#16#male#19/August/2004#destini you might not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0#16#male#17/August/2004#dear you say it or yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0#16#male#16/August/2004#main aur meri tanhai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0#16#male#14/August/2004#mail addressr s urlli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0#16#male#09/August/2004#allris so stand back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0#16#male#09/August/2004#miss you badli i am l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0#16#male#07/August/2004#hazel eye close your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0#16#male#07/August/2004#let it be me a bird h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1#25#female#29/May/2004#it been a long time co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1#25#female#30/June/2004#urllink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1#25#female#29/June/2004#yeah i wa a leeeeetl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1#25#female#28/June/2004#urllink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1#25#female#28/June/2004#happi birthday to my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1#25#female#27/June/2004#damn lasanga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1#25#female#26/June/2004#urllink what find nem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1#25#female#26/June/2004#i make excel lasagna ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1#25#female#25/June/2004#oh what a busi fuck w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1#25#female#21/June/2004#so jefe come in thi m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1#25#female#20/June/2004#what is your battl cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1#25#female#20/June/2004#it been a long weeken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1#25#female#17/June/2004#i tri to do my person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1#25#female#15/June/2004#it day like thi that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1#25#female#15/June/2004#convers of the day po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1#25#female#14/June/2004#the ir suck major ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1#25#female#12/June/2004#oh my ach head it alm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1#25#female#11/June/2004#i at lisa i just had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1#25#female#09/June/2004#i ca find a titl i li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1#25#female#08/June/2004#i have spent the enti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1#25#female#07/June/2004#oh what a day the tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1#25#female#04/June/2004#so here the kink in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681258</th>\n",
       "      <td>19319#23#male#07/March/2004#hello friend i am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681259</th>\n",
       "      <td>19319#23#male#04/March/2004#a barrel of monkey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681260</th>\n",
       "      <td>19319#23#male#29/April/2004#wnat to read my la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681261</th>\n",
       "      <td>19319#23#male#26/April/2004#truer word were ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681262</th>\n",
       "      <td>19319#23#male#26/April/2004#red beard time tic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681263</th>\n",
       "      <td>19319#23#male#11/April/2004#shatter ass veri l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681264</th>\n",
       "      <td>19319#23#male#03/April/2004#nobodi read thi so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681265</th>\n",
       "      <td>19319#23#male#01/April/2004#addendum sorri for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681266</th>\n",
       "      <td>19319#23#male#01/April/2004#everybodi puke we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681267</th>\n",
       "      <td>19319#23#male#01/April/2004#what do they want ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681268</th>\n",
       "      <td>19319#23#male#26/May/2004#eboni pene littl bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681269</th>\n",
       "      <td>19319#23#male#24/May/2004#sympathi for the dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681270</th>\n",
       "      <td>19319#23#male#22/May/2004#tucmcari new mexico ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681271</th>\n",
       "      <td>19319#23#male#18/May/2004#drive across jersey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681272</th>\n",
       "      <td>19319#23#male#14/May/2004#bumfight ye it wrong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681273</th>\n",
       "      <td>19319#23#male#12/May/2004#captain beefheart or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681274</th>\n",
       "      <td>19319#23#male#06/May/2004#it movi time there a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681275</th>\n",
       "      <td>19319#23#male#06/May/2004#i tell ya a quicki t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681276</th>\n",
       "      <td>19319#23#male#02/May/2004#a few more dollar th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681277</th>\n",
       "      <td>19319#23#male#03/June/2004#maxwel ever listen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681278</th>\n",
       "      <td>19319#23#male#31/July/2004#in the jungl babi w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681279</th>\n",
       "      <td>19319#23#male#27/July/2004#the littl thing whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681280</th>\n",
       "      <td>19319#23#male#25/July/2004#mutini one of the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681281</th>\n",
       "      <td>19319#23#male#21/July/2004#a good day today sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681282</th>\n",
       "      <td>19319#23#male#19/July/2004#so far i just have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681283</th>\n",
       "      <td>19319#23#male#07/July/2004#quit sometim welcom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681284</th>\n",
       "      <td>19319#23#male#12/August/2004#thing more import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681285</th>\n",
       "      <td>19319#23#male#06/August/2004#movi about movi w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681286</th>\n",
       "      <td>19319#23#male#05/August/2004#movi about movi w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681287</th>\n",
       "      <td>19319#23#male#05/August/2004#morgan wixson the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>681288 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              author#age#gender#date#post\n",
       "0       0#16#male#19/August/2004#destini you might not...\n",
       "1       0#16#male#17/August/2004#dear you say it or yo...\n",
       "2       0#16#male#16/August/2004#main aur meri tanhai ...\n",
       "3       0#16#male#14/August/2004#mail addressr s urlli...\n",
       "4       0#16#male#09/August/2004#allris so stand back ...\n",
       "5       0#16#male#09/August/2004#miss you badli i am l...\n",
       "6       0#16#male#07/August/2004#hazel eye close your ...\n",
       "7       0#16#male#07/August/2004#let it be me a bird h...\n",
       "8       1#25#female#29/May/2004#it been a long time co...\n",
       "9                        1#25#female#30/June/2004#urllink\n",
       "10      1#25#female#29/June/2004#yeah i wa a leeeeetl ...\n",
       "11                       1#25#female#28/June/2004#urllink\n",
       "12      1#25#female#28/June/2004#happi birthday to my ...\n",
       "13                  1#25#female#27/June/2004#damn lasanga\n",
       "14      1#25#female#26/June/2004#urllink what find nem...\n",
       "15      1#25#female#26/June/2004#i make excel lasagna ...\n",
       "16      1#25#female#25/June/2004#oh what a busi fuck w...\n",
       "17      1#25#female#21/June/2004#so jefe come in thi m...\n",
       "18      1#25#female#20/June/2004#what is your battl cr...\n",
       "19      1#25#female#20/June/2004#it been a long weeken...\n",
       "20      1#25#female#17/June/2004#i tri to do my person...\n",
       "21      1#25#female#15/June/2004#it day like thi that ...\n",
       "22      1#25#female#15/June/2004#convers of the day po...\n",
       "23      1#25#female#14/June/2004#the ir suck major ass...\n",
       "24      1#25#female#12/June/2004#oh my ach head it alm...\n",
       "25      1#25#female#11/June/2004#i at lisa i just had ...\n",
       "26      1#25#female#09/June/2004#i ca find a titl i li...\n",
       "27      1#25#female#08/June/2004#i have spent the enti...\n",
       "28      1#25#female#07/June/2004#oh what a day the tri...\n",
       "29      1#25#female#04/June/2004#so here the kink in t...\n",
       "...                                                   ...\n",
       "681258  19319#23#male#07/March/2004#hello friend i am ...\n",
       "681259  19319#23#male#04/March/2004#a barrel of monkey...\n",
       "681260  19319#23#male#29/April/2004#wnat to read my la...\n",
       "681261  19319#23#male#26/April/2004#truer word were ne...\n",
       "681262  19319#23#male#26/April/2004#red beard time tic...\n",
       "681263  19319#23#male#11/April/2004#shatter ass veri l...\n",
       "681264  19319#23#male#03/April/2004#nobodi read thi so...\n",
       "681265  19319#23#male#01/April/2004#addendum sorri for...\n",
       "681266  19319#23#male#01/April/2004#everybodi puke we ...\n",
       "681267  19319#23#male#01/April/2004#what do they want ...\n",
       "681268  19319#23#male#26/May/2004#eboni pene littl bil...\n",
       "681269  19319#23#male#24/May/2004#sympathi for the dev...\n",
       "681270  19319#23#male#22/May/2004#tucmcari new mexico ...\n",
       "681271  19319#23#male#18/May/2004#drive across jersey ...\n",
       "681272  19319#23#male#14/May/2004#bumfight ye it wrong...\n",
       "681273  19319#23#male#12/May/2004#captain beefheart or...\n",
       "681274  19319#23#male#06/May/2004#it movi time there a...\n",
       "681275  19319#23#male#06/May/2004#i tell ya a quicki t...\n",
       "681276  19319#23#male#02/May/2004#a few more dollar th...\n",
       "681277  19319#23#male#03/June/2004#maxwel ever listen ...\n",
       "681278  19319#23#male#31/July/2004#in the jungl babi w...\n",
       "681279  19319#23#male#27/July/2004#the littl thing whe...\n",
       "681280  19319#23#male#25/July/2004#mutini one of the t...\n",
       "681281  19319#23#male#21/July/2004#a good day today sh...\n",
       "681282  19319#23#male#19/July/2004#so far i just have ...\n",
       "681283  19319#23#male#07/July/2004#quit sometim welcom...\n",
       "681284  19319#23#male#12/August/2004#thing more import...\n",
       "681285  19319#23#male#06/August/2004#movi about movi w...\n",
       "681286  19319#23#male#05/August/2004#movi about movi w...\n",
       "681287  19319#23#male#05/August/2004#morgan wixson the...\n",
       "\n",
       "[681288 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.read_csv('data/blogs_clean.csv')\n",
    "print(len(data_clean))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convertXML2(name):\n",
    "\n",
    "    with open (\"data/blogs/{}\".format(name), \"r\",encoding = \"ISO-8859-1\") as myfile:\n",
    "        data=myfile.read().replace('\\n', '')\n",
    "\n",
    "    obj = xmltodict.parse(data)\n",
    "    blogs_json = json.dumps(obj)\n",
    "    blogs_json = json.loads(blogs_json)\n",
    "\n",
    "    date = blogs_json['Blog']['date']\n",
    "    post = blogs_json['Blog']['post']\n",
    "\n",
    "    for i in zip(date,post):\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = titles[1]\n",
    "with open (\"data/blogs/{}\".format(name), \"r\") as myfile:\n",
    "        data=myfile.read().replace('\\n', '')\n",
    "xml = '<root>' + data + '</root>'\n",
    "\n",
    "obj = xmltodict.parse(xml)\n",
    "blogs_json = json.dumps(obj)\n",
    "blogs_json = json.loads(blogs_json)\n",
    "\n",
    "#convertXML(xml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
